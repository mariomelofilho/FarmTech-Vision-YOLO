{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c71005c",
   "metadata": {},
   "source": [
    "\n",
    "# **FIAP – Fase 6 (Parte 2) – Comparação YOLO vs CNN**\n",
    "**FarmTech Solutions – O Começo da Rede Neural**  \n",
    "Autor: *preencha aqui* | RM: *preencha aqui*  \n",
    "Data de geração deste notebook: 2025-10-14 03:58:35  \n",
    "\n",
    "> **Objetivo:** Comparar três abordagens de visão computacional usando a mesma base da Entrega 1:  \n",
    "> 1) **YOLOv5 Adaptável** (*pesos treinados na Entrega 1*)  \n",
    "> 2) **YOLOv5 Padrão** (*treino baseline com hiperparâmetros default*)  \n",
    "> 3) **CNN do zero (classificação)** (*a partir das imagens e rótulos da YOLO*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b0156",
   "metadata": {},
   "source": [
    "\n",
    "## ✅ **Checklist de Pré-Requisitos**\n",
    "- Sua base no **Google Drive** organizada conforme a Entrega 1 (imagens e rótulos YOLO-format).\n",
    "- Um arquivo `dataset.yaml` no padrão YOLO com caminhos relativos para `train`, `val`, `test`, e a lista de `names` das classes.\n",
    "- Os **pesos** treinados na Entrega 1 (por exemplo, `runs/train/expX/weights/best.pt`) salvos no Drive.\n",
    "- Colab com GPU ativada (*Runtime > Change runtime type > GPU*).\n",
    "\n",
    "> Se seu dataset tiver **apenas um objeto por imagem**, a conversão para classificação é direta. Caso haja múltiplos objetos por imagem, este notebook **seleciona a classe dominante** (maior número de anotações) para fins de classificação. Imagens com **empate** podem ser **ignoradas** para a CNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9590d9c",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Ambiente: Montagem do Drive e Instalação de Dependências\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3ed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Montar Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"✅ Drive montado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66821f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Instalar YOLOv5 e dependências PyTorch/Ultralytics\n",
    "!pip -q install --upgrade pip\n",
    "!pip -q install ultralytics==8.2.103  # para utilidades e inferências\n",
    "!git clone -q https://github.com/ultralytics/yolov5.git\n",
    "%cd yolov5\n",
    "!pip -q install -r requirements.txt\n",
    "%cd /content\n",
    "print(\"✅ YOLOv5 instalado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36811593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Instalar dependências para a CNN (TensorFlow/Keras) e métricas\n",
    "!pip -q install tensorflow==2.16.1 scikit-learn==1.5.2 matplotlib==3.8.4 pandas==2.2.2\n",
    "print(\"✅ TensorFlow/Keras e libs instaladas.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1900d2d",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Variáveis de Caminho (Ajuste para seu Drive)\n",
    "Preencha os caminhos abaixo antes de rodar o notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4193d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title 🔧 Configuração de caminhos (EDITE AQUI)\n",
    "from pathlib import Path\n",
    "\n",
    "# Caminho do YAML do dataset YOLO (no Drive)\n",
    "DATASET_YAML_PATH = \"/content/drive/MyDrive/fase6/dataset/dataset.yaml\"  #@param {type:\"string\"}\n",
    "\n",
    "# Caminho dos pesos treinados na Entrega 1 (YOLO adaptável)\n",
    "CUSTOM_WEIGHTS_PATH = \"/content/drive/MyDrive/fase6/yolo_runs/exp/weights/best.pt\"  #@param {type:\"string\"}\n",
    "\n",
    "# Pasta para salvar saídas desta Parte 2\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/fase6/entrega2_outputs\"  #@param {type:\"string\"}\n",
    "\n",
    "# Pasta temporária (local) para classificação (gerada a partir de rótulos YOLO)\n",
    "CLASS_DATASET_DIR = \"/content/classification_ds\"\n",
    "\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(\"✅ Config aplicado.\")\n",
    "print(\"DATASET_YAML_PATH:\", DATASET_YAML_PATH)\n",
    "print(\"CUSTOM_WEIGHTS_PATH:\", CUSTOM_WEIGHTS_PATH)\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58db946",
   "metadata": {},
   "source": [
    "\n",
    "## 3) YOLOv5 Padrão (Baseline) – Treino e Validação\n",
    "Treinaremos a YOLOv5s com **hiperparâmetros padrão** (sem ajustes finos), para comparar com o modelo customizado da Entrega 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d56e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Treinar YOLOv5 (baseline - hiperparâmetros padrão)\n",
    "import os, json, shutil, time, sys\n",
    "\n",
    "%cd /content/yolov5\n",
    "!python train.py --img 640 --batch 16 --epochs 30 --data \"$DATASET_YAML_PATH\" --weights yolov5s.pt --name fase6_yolo_baseline --project /content/yolo_runs\n",
    "\n",
    "# Registrar o caminho do experimento\n",
    "BASELINE_EXP_DIR = \"/content/yolo_runs/fase6_yolo_baseline\"\n",
    "print(\"✅ Treino baseline finalizado em:\", BASELINE_EXP_DIR)\n",
    "%cd /content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfd811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Avaliar YOLOv5 Baseline no conjunto de teste\n",
    "%cd /content/yolov5\n",
    "!python val.py --weights \"/content/yolo_runs/fase6_yolo_baseline/weights/best.pt\" --data \"$DATASET_YAML_PATH\" --task test --img 640 --name fase6_yolo_baseline_test --project /content/yolo_val\n",
    "print(\"✅ Avaliação baseline concluída.\")\n",
    "%cd /content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d661b9",
   "metadata": {},
   "source": [
    "\n",
    "## 4) YOLOv5 Adaptável (Pesos da Entrega 1) – Avaliação\n",
    "Carregamos os **pesos customizados** (melhor epoch da Entrega 1) e avaliamos no conjunto de teste para comparação direta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec507091",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Avaliar YOLOv5 (pesos customizados da Entrega 1) no teste\n",
    "from pathlib import Path\n",
    "\n",
    "assert Path(CUSTOM_WEIGHTS_PATH).exists(), \"❌ CUSTOM_WEIGHTS_PATH não encontrado.\"\n",
    "%cd /content/yolov5\n",
    "!python val.py --weights \"$CUSTOM_WEIGHTS_PATH\" --data \"$DATASET_YAML_PATH\" --task test --img 640 --name fase6_yolo_custom_test --project /content/yolo_val\n",
    "print(\"✅ Avaliação YOLO customizada concluída.\")\n",
    "%cd /content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ead8cf",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Conversão do Dataset YOLO → Classificação (2 classes)\n",
    "A CNN requer pastas por classe (`train/classA`, `train/classB`, ...).  \n",
    "Este passo usa os **arquivos de rótulo YOLO** para determinar a **classe dominante** da imagem e cria uma cópia em uma estrutura de diretórios de classificação.\n",
    "\n",
    "> Regras:  \n",
    "> - Se a imagem tiver **uma única classe**, usa-se aquela.  \n",
    "> - Se tiver **várias classes**, escolhe-se a **mais frequente** na imagem.  \n",
    "> - Se houver **empate**, a imagem é **ignoradas** para a CNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f353627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Conversão rótulos YOLO → dataset de classificação\n",
    "import os, shutil, yaml, glob\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "CLASS_DATASET_DIR = Path(\"/content/classification_ds\")\n",
    "if CLASS_DATASET_DIR.exists():\n",
    "    shutil.rmtree(CLASS_DATASET_DIR)\n",
    "CLASS_DATASET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(DATASET_YAML_PATH, 'r') as f:\n",
    "    ds = yaml.safe_load(f)\n",
    "\n",
    "# ds['train'], ds['val'], ds['test'] podem ser pastas ou arquivos .txt com listas.\n",
    "def resolve_image_paths(entry):\n",
    "    p = Path(entry)\n",
    "    if p.suffix.lower() == \".txt\":\n",
    "        with open(p) as fp:\n",
    "            return [Path(line.strip()) for line in fp if line.strip()]\n",
    "    else:\n",
    "        # assume diretório com imagens\n",
    "        exts = (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\")\n",
    "        paths = []\n",
    "        for ext in exts:\n",
    "            paths.extend(Path(p).rglob(ext))\n",
    "        return paths\n",
    "\n",
    "splits = {'train': ds['train'], 'val': ds.get('val', None), 'test': ds.get('test', None)}\n",
    "names = ds.get('names', None)\n",
    "if isinstance(names, dict):\n",
    "    # YOLO às vezes usa dict {0:'A',1:'B'}\n",
    "    class_names = [names[k] for k in sorted(names.keys(), key=int)]\n",
    "else:\n",
    "    class_names = list(names)\n",
    "\n",
    "def yolo_label_path(img_path):\n",
    "    # YOLO: images/ -> labels/, ext -> .txt\n",
    "    img_path = Path(img_path)\n",
    "    if \"images\" in img_path.parts:\n",
    "        idx = img_path.parts.index(\"images\")\n",
    "        lbl_parts = list(img_path.parts)\n",
    "        lbl_parts[idx] = \"labels\"\n",
    "        lbl_path = Path(*lbl_parts).with_suffix(\".txt\")\n",
    "        return lbl_path\n",
    "    else:\n",
    "        # fallback: assume pasta paralela labels com mesmo basename\n",
    "        return img_path.with_suffix(\".txt\").parent.parent / \"labels\" / (img_path.stem + \".txt\")\n",
    "\n",
    "def dominant_class(label_file):\n",
    "    if not label_file.exists():\n",
    "        return None\n",
    "    cls_ids = []\n",
    "    with open(label_file) as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if not parts:\n",
    "                continue\n",
    "            cls_ids.append(int(parts[0]))\n",
    "    if not cls_ids:\n",
    "        return None\n",
    "    count = Counter(cls_ids)\n",
    "    most_common = count.most_common()\n",
    "    if len(most_common) == 1 or (len(most_common) > 1 and most_common[0][1] > most_common[1][1]):\n",
    "        return most_common[0][0]\n",
    "    return None  # empate\n",
    "\n",
    "def copy_for_split(split_name, sources):\n",
    "    if not sources:\n",
    "        return 0, 0\n",
    "    kept, skipped = 0, 0\n",
    "    for img in sources:\n",
    "        lbl = yolo_label_path(img)\n",
    "        cls_id = dominant_class(lbl)\n",
    "        if cls_id is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        cls_name = class_names[cls_id] if class_names and cls_id < len(class_names) else f\"class_{cls_id}\"\n",
    "        out_dir = CLASS_DATASET_DIR / split_name / cls_name\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy2(img, out_dir / img.name)\n",
    "        kept += 1\n",
    "    return kept, skipped\n",
    "\n",
    "stats = {}\n",
    "for split_name, entry in splits.items():\n",
    "    if entry is None:\n",
    "        continue\n",
    "    imgs = resolve_image_paths(entry)\n",
    "    kept, skipped = copy_for_split(split_name, imgs)\n",
    "    stats[split_name] = dict(kept=kept, skipped=skipped)\n",
    "\n",
    "print(\"✅ Conversão concluída.\")\n",
    "print(stats)\n",
    "print(\"Estrutura criada em:\", CLASS_DATASET_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6913096b",
   "metadata": {},
   "source": [
    "\n",
    "## 6) CNN do Zero (Keras/TensorFlow) – Treino e Avaliação\n",
    "Arquitetura simples: `Conv2D → MaxPool → Conv2D → MaxPool → Flatten → Dense → Dropout → Dense(softmax)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d91ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Preparar data loaders (ImageDataGenerator)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "IMG_SIZE = 224  # resolução para a CNN\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=True, rotation_range=10, width_shift_range=0.1, height_shift_range=0.1)\n",
    "val_datagen   = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen  = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    directory=\"/content/classification_ds/train\",\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_gen = val_datagen.flow_from_directory(\n",
    "    directory=\"/content/classification_ds/val\",\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_gen = test_datagen.flow_from_directory(\n",
    "    directory=\"/content/classification_ds/test\",\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "NUM_CLASSES = train_gen.num_classes\n",
    "print(\"Classes:\", train_gen.class_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe4fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Definir e treinar a CNN\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_cnn(input_shape=(224,224,3), num_classes=2):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(128, (3,3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "cnn = build_cnn(num_classes=NUM_CLASSES)\n",
    "cnn.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "cnn.summary()\n",
    "\n",
    "EPOCHS = 30  # baseline\n",
    "history = cnn.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "# Salvar pesos\n",
    "cnn.save(\"/content/cnn_baseline.h5\")\n",
    "print(\"✅ CNN treinada e salva.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df917dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Avaliar CNN no conjunto de teste (métricas de classificação)\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Predições\n",
    "test_gen.reset()\n",
    "t0 = time.time()\n",
    "probs = cnn.predict(test_gen, verbose=0)\n",
    "latency = (time.time() - t0) / max(1, test_gen.n)  # s/imagem\n",
    "y_pred = np.argmax(probs, axis=1)\n",
    "y_true = test_gen.classes\n",
    "labels = list(test_gen.class_indices.keys())\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision (weighted):\", prec)\n",
    "print(\"Recall (weighted):\", rec)\n",
    "print(\"F1 (weighted):\", f1)\n",
    "print(\"Latency (s/img):\", latency)\n",
    "\n",
    "# Salvar relatório\n",
    "report = classification_report(y_true, y_pred, target_names=labels, zero_division=0, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report.to_csv(f\"{OUTPUT_DIR}/cnn_classification_report.csv\", index=True)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(4,4))\n",
    "import itertools\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title(\"CNN - Matriz de Confusão\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(labels))\n",
    "plt.xticks(tick_marks, labels, rotation=45)\n",
    "plt.yticks(tick_marks, labels)\n",
    "\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/cnn_confusion_matrix.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Salvar métricas resumidas\n",
    "summary = {\n",
    "    \"accuracy\": float(acc),\n",
    "    \"precision_weighted\": float(prec),\n",
    "    \"recall_weighted\": float(rec),\n",
    "    \"f1_weighted\": float(f1),\n",
    "    \"latency_s_per_image\": float(latency)\n",
    "}\n",
    "import json, os\n",
    "with open(f\"{OUTPUT_DIR}/cnn_metrics.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(\"✅ Métricas CNN salvas em\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fba1ddd",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Medir Latência de Inferência (YOLO vs CNN) e Consolidar Métricas\n",
    "Executamos inferência na **pasta de teste** e registramos tempo médio por imagem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb86bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Funções auxiliares para inferência YOLO em lote\n",
    "import os, time, glob, json, shutil, yaml, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def list_test_images_from_yaml(yaml_path):\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        ds = yaml.safe_load(f)\n",
    "    test_entry = ds.get('test')\n",
    "    if not test_entry:\n",
    "        raise ValueError(\"YAML não possui entrada 'test'.\")\n",
    "    p = Path(test_entry)\n",
    "    if p.suffix.lower() == \".txt\":\n",
    "        with open(p) as fp:\n",
    "            return [Path(line.strip()) for line in fp if line.strip()]\n",
    "    else:\n",
    "        exts = (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\")\n",
    "        paths = []\n",
    "        for ext in exts:\n",
    "            paths.extend(Path(p).rglob(ext))\n",
    "        return paths\n",
    "\n",
    "TEST_IMAGES = list_test_images_from_yaml(DATASET_YAML_PATH)\n",
    "len(TEST_IMAGES), TEST_IMAGES[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9f9bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Inferência YOLOv5 baseline (tempo médio)\n",
    "import time\n",
    "%cd /content/yolov5\n",
    "t0 = time.time()\n",
    "!python detect.py --weights \"/content/yolo_runs/fase6_yolo_baseline/weights/best.pt\" --source \"{' '.join(str(p) for p in TEST_IMAGES)}\" --img 640 --save-txt --save-conf --exist-ok --project /content/yolo_detect --name baseline\n",
    "t1 = time.time()\n",
    "baseline_latency = (t1 - t0) / max(1, len(TEST_IMAGES))\n",
    "print(\"Latency baseline (s/img):\", baseline_latency)\n",
    "%cd /content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e4492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Inferência YOLOv5 custom (tempo médio)\n",
    "import time\n",
    "%cd /content/yolov5\n",
    "t0 = time.time()\n",
    "!python detect.py --weights \"$CUSTOM_WEIGHTS_PATH\" --source \"{' '.join(str(p) for p in TEST_IMAGES)}\" --img 640 --save-txt --save-conf --exist-ok --project /content/yolo_detect --name custom\n",
    "t1 = time.time()\n",
    "custom_latency = (t1 - t0) / max(1, len(TEST_IMAGES))\n",
    "print(\"Latency custom (s/img):\", custom_latency)\n",
    "%cd /content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9347a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Consolidar métricas (YOLO baseline, YOLO custom, CNN)\n",
    "import json, pandas as pd, glob\n",
    "\n",
    "# Ler métricas val/test da YOLO (mAP etc.) a partir dos resultados gerados por yolov5 (results.txt)\n",
    "def read_yolo_results_txt(folder_glob):\n",
    "    # Procura o arquivo results.txt mais recente nos diretórios correspondentes\n",
    "    files = sorted(glob.glob(folder_glob, recursive=True), key=os.path.getmtime, reverse=True)\n",
    "    metrics = {}\n",
    "    for f in files:\n",
    "        if os.path.basename(f) == \"results.txt\":\n",
    "            with open(f) as fp:\n",
    "                lines = fp.readlines()\n",
    "            # Na última linha normalmente há \"all\" com mAP50, mAP50-95 etc., mas o formato pode variar por versão\n",
    "            # Vamos guardar todo o arquivo como texto bruto também.\n",
    "            metrics[\"raw\"] = \"\".join(lines[-5:])\n",
    "            break\n",
    "    return metrics\n",
    "\n",
    "baseline_metrics = read_yolo_results_txt(\"/content/yolo_val/fase6_yolo_baseline_test/**/results.txt\")\n",
    "custom_metrics   = read_yolo_results_txt(\"/content/yolo_val/fase6_yolo_custom_test/**/results.txt\")\n",
    "\n",
    "summary = {\n",
    "    \"yolo_baseline\": {\n",
    "        \"val_text_tail\": baseline_metrics.get(\"raw\", \"\"),\n",
    "        \"latency_s_per_image\": globals().get(\"baseline_latency\", None)\n",
    "    },\n",
    "    \"yolo_custom\": {\n",
    "        \"val_text_tail\": custom_metrics.get(\"raw\", \"\"),\n",
    "        \"latency_s_per_image\": globals().get(\"custom_latency\", None)\n",
    "    },\n",
    "}\n",
    "\n",
    "# Acrescentar também as métricas da CNN\n",
    "with open(f\"{OUTPUT_DIR}/cnn_metrics.json\", \"r\") as f:\n",
    "    cnn_m = json.load(f)\n",
    "summary[\"cnn\"] = cnn_m\n",
    "\n",
    "# Salvar JSON consolidado\n",
    "with open(f\"{OUTPUT_DIR}/comparativo_metrics.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "import pandas as pd\n",
    "df_summary = pd.DataFrame({\n",
    "    \"Modelo\": [\"YOLOv5 Baseline\", \"YOLOv5 Custom\", \"CNN (Keras)\"],\n",
    "    \"Precisão/Notas\": [\n",
    "        \"Ver tail do results.txt baseline (mAP)\",\n",
    "        \"Ver tail do results.txt custom (mAP)\",\n",
    "        f\"Acc={cnn_m.get('accuracy', None):.3f} | F1={cnn_m.get('f1_weighted', None):.3f}\"\n",
    "    ],\n",
    "    \"Latência (s/img)\": [\n",
    "        summary[\"yolo_baseline\"][\"latency_s_per_image\"],\n",
    "        summary[\"yolo_custom\"][\"latency_s_per_image\"],\n",
    "        summary[\"cnn\"][\"latency_s_per_image\"]\n",
    "    ]\n",
    "})\n",
    "\n",
    "df_summary.to_csv(f\"{OUTPUT_DIR}/comparativo_metrics_table.csv\", index=False)\n",
    "print(\"✅ Tabela comparativa salva em CSV.\")\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a98447",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Discussão e Conclusões (preencha após execução)\n",
    "- **Facilidade de uso/integração:**  \n",
    "  - YOLOv5 baseline: *comente aqui.*  \n",
    "  - YOLOv5 custom: *comente aqui.*  \n",
    "  - CNN do zero: *comente aqui.*  \n",
    "\n",
    "- **Precisão do modelo (mAP/Accuracy/F1):**  \n",
    "  - YOLOv5 baseline: *X*  \n",
    "  - YOLOv5 custom: *Y*  \n",
    "  - CNN (acc/f1): *Z*  \n",
    "\n",
    "- **Tempo de treinamento/customização:**  \n",
    "  - YOLOv5 baseline: *X min*  \n",
    "  - YOLOv5 custom: *Y min/épocas*  \n",
    "  - CNN: *Z min*  \n",
    "\n",
    "- **Tempo de inferência (s/imagem):**  \n",
    "  - YOLOv5 baseline: *X*  \n",
    "  - YOLOv5 custom: *Y*  \n",
    "  - CNN: *Z*  \n",
    "\n",
    "> **Conclusão:** Relacione os resultados com o **cenário de aplicação** (segurança patrimonial, controle de acessos, etc.). Normalmente a **YOLO custom** tende a oferecer melhor mAP e latência menor que uma CNN simples para tarefas de **detecção**. A CNN pode ser competitiva para **classificação** pura, com custo de implementação baixo, mas sem fornecer bounding boxes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae5ff1",
   "metadata": {},
   "source": [
    "\n",
    "## Apêndice – Reprodutibilidade\n",
    "- Configure seeds se desejar reprodutibilidade estrita (pode impactar performance):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07860b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title (Opcional) Fixar seeds\n",
    "import os, random, numpy as np, torch, tensorflow as tf\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "print(\"✅ Seeds configuradas (parcial).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8150dda3",
   "metadata": {},
   "source": [
    "\n",
    "## Artefatos Gerados\n",
    "- `yolo_runs/fase6_yolo_baseline/...` (treino YOLO baseline)  \n",
    "- `yolo_val/fase6_yolo_baseline_test/...` (val YOLO baseline)  \n",
    "- `yolo_val/fase6_yolo_custom_test/...` (val YOLO custom)  \n",
    "- `classification_ds/` (dataset convertido para classificação)  \n",
    "- `cnn_baseline.h5` (pesos CNN)  \n",
    "- `comparativo_metrics.json`, `comparativo_metrics_table.csv`, `cnn_metrics.json`, `cnn_confusion_matrix.png` (em `OUTPUT_DIR`)  \n",
    "\n",
    "Inclua prints das detecções em `/content/yolo_detect/` no seu **README** e **vídeo**.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
